{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d35bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import split, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0c2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new spark session \n",
    "spark = SparkSession.builder.appName('PPA detection').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee2507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection to cassandra database and cnas keyspace\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect('cnas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4daf88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get parameters (start_date and end_date)\n",
    "import sys\n",
    "\n",
    "date_debut=sys.argv[1]\n",
    "date_fin = sys.argv[2]\n",
    "\n",
    "query = \"SELECT *  FROM cnas  WHERE date_paiement >= '{}' AND date_paiement <= '{}' LIMIT 10  ALLOW FILTERING;\".format(date_debut,date_fin)\n",
    "rows = session.execute(query)\n",
    "#print the data : print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b87b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the cassandra.cluster ( rows) to pandas dataframe to make some changes\n",
    "dftable = pd.DataFrame(list(rows))\n",
    "# print the data : print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13972234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation :\n",
    "\n",
    "#remplacer None avec -1 ( aucune affection) dans la coloumn affection\n",
    "dftable.affection.fillna(value=-1, inplace=True)\n",
    "\n",
    "#remplacer None avec 0 ( aucune quantitée rejetée ) dans la coloumn qte_rejet\n",
    "dftable.qte_rejet.fillna(value=0, inplace=True)\n",
    "\n",
    "\n",
    "# delete rows where the quantite_med == 0\n",
    "dftable.drop(dftable[dftable['quantite_med'] == 0].index, inplace = True)\n",
    "\n",
    "#delete rejected quantity , but before this , we need to save the rows having quantity rejected > 0\n",
    "#df_rejected = dftable[dftable['qte_rejet'] > 0]\n",
    "#dftable.drop(dftable[dftable['qte_rejet'] > 0].index, inplace = True)\n",
    "\n",
    "\n",
    "#remplacer None avec 0 ( aucune durée spécifiée ) dans la coloumn duree_traitement\n",
    "dftable.duree_traitement.fillna(value=0, inplace=True)\n",
    "\n",
    "#change the type of some lines \n",
    "dftable = dftable.astype({\"affection\": str})\n",
    "dftable = dftable.astype({\"fk\": float})\n",
    "dftable = dftable.astype({\"age\": int})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# garder les coloumns qu'on est besoin \n",
    "dftable=dftable[['id','fk','codeps','affection','age','applic_tarif','date_paiement','num_enr','sexe','ts','quantite_med','qte_rejet']]\n",
    "# print the columns that we need : dftable.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4646bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the table into two table : rejected one and accepted one\n",
    "rejected = dftable[dftable['qte_rejet'] > 0]\n",
    "accepted = dftable[dftable['qte_rejet'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc619651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark dataframe for the two pandas table (accepted and rejected)\n",
    "sparkdf = spark.createDataFrame(accepted)\n",
    "rejected_sparkdf = spark.createDataFrame(rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the affection column to array of int ( splited by ',')\n",
    "sparkdf = sparkdf.withColumn(\"affection\", split(col(\"affection\"), \",\").cast(\"array<int>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the affection array \n",
    "import pyspark.sql.functions as F\n",
    "sparkdf = sparkdf.withColumn('affection', F.array_sort('affection'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "## put the age in ranges\n",
    "from pyspark.sql.functions import udf\n",
    "@udf(\"String\")\n",
    "def age_range(age):\n",
    "    if age >= 0 and age <= 5:\n",
    "        return '0-5'\n",
    "    elif age > 5 and age <= 10:\n",
    "        return '6-10'\n",
    "    elif age > 10 and age <= 16:\n",
    "        return '11-16' \n",
    "    elif age > 16 and age <= 24:\n",
    "        return '17-24' \n",
    "    elif age > 24 and age <= 60:\n",
    "        return '25-60' \n",
    "    elif age > 60 and age <= 76:\n",
    "        return '61-76' \n",
    "    else:\n",
    "        return '75+'\n",
    "    \n",
    "\n",
    "\n",
    "sparkdf = sparkdf.withColumn(\"age\", age_range(col(\"age\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645061bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the affection column to a string again ( so we can index it)\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "sparkdf = sparkdf.withColumn(\"affection\",\n",
    "   concat_ws(\",\",col(\"affection\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handling Categorical Features\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer=StringIndexer(inputCols=[\"sexe\",\"applic_tarif\",\"ts\",\"affection\",\"age\"],outputCols=[\"sex_indexed\",\"applic_tarif_indexed\",\n",
    "                                                                         \"ts_indexes\",\"affection_indexes\",\"age_indexes\"])\n",
    "df_r=indexer.setHandleInvalid(\"keep\").fit(sparkdf).transform(sparkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data into one vector \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=['id','fk','age_indexes','sex_indexed','affection_indexes',\n",
    "                          'ts_indexes','quantite_med',],outputCol=\"Independent Features\")\n",
    "output=featureassembler.transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data to fit it to the model \n",
    "finalized_data=output.select(\"Independent Features\",\"quantite_med\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call and fit the model \n",
    "from pyspark.ml.regression import LinearRegression\n",
    "##train test split\n",
    "train_data,test_data=finalized_data.randomSplit([0.75,0.25])\n",
    "regressor=LinearRegression(featuresCol='Independent Features', labelCol='quantite_med', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c972d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "print(\"Coefficients: %s\" % str(regressor.coefficients))\n",
    "print(\"Intercept: %s\" % str(regressor.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictions\n",
    "pred_results=regressor.evaluate(test_data)\n",
    "## Final comparison\n",
    "# print the prediction vs the vector : pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performance Metrics\n",
    "pred_results.r2,pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final predicted quantity ( rounded ) vs the real quantity\n",
    "Final_result = pred_results.predictions.where(\"quantite_med > prediction \")\n",
    "\n",
    "from pyspark.sql.functions import round, col\n",
    "Final_result = Final_result.select(\"Independent Features\"  , \"quantite_med\", round(col('prediction')))\n",
    "Final_result.show(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
